{"id": "9474", "post": "<div class=\"post-9474 post type-post status-publish format-standard hentry category-tutorials tag-scrapy tag-ubuntu tag-web-crawler\" id=\"post-9474\"><h2 class=\"storytitle\"><a href=\"https://lowendbox.com/blog/how-to-build-your-own-web-crawler-using-an-ubuntu-vps/\" rel=\"bookmark\">How to Build Your Own Web Crawler Using an Ubuntu VPS</a></h2><div class=\"meta\"><img data-lazyloaded=\"1\" src=\"data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=\" data-src=\"/media/icons/tag_green.png\" alt=\"Tags: \"> <a href=\"https://lowendbox.com/tag/scrapy/\" rel=\"tag\">scrapy</a>, <a href=\"https://lowendbox.com/tag/ubuntu/\" rel=\"tag\">ubuntu</a>, <a href=\"https://lowendbox.com/tag/web-crawler/\" rel=\"tag\">web crawler</a> <img data-lazyloaded=\"1\" src=\"data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=\" data-src=\"https://lowendbox.com/media/icons/calendar.png\" alt=\"Date/Time: \"> December 24, 2016 @ 6:53 am, by Jon Biloh</div><div class=\"storycontent tablelook\"><p style=\"text-align: center;\"><a href=\"https://lowendbox.com/wp-content/uploads/2013/05/lowendtutorial.png\"><img data-lazyloaded=\"1\" src=\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNzEiIGhlaWdodD0iNzAiIHZpZXdCb3g9IjAgMCAyNzEgNzAiPjxyZWN0IHdpZHRoPSIxMDAlIiBoZWlnaHQ9IjEwMCUiIGZpbGw9IiNjZmQ0ZGIiLz48L3N2Zz4=\" loading=\"lazy\" class=\"alignnone size-full wp-image-3957\" data-src=\"https://lowendbox.com/wp-content/uploads/2013/05/lowendtutorial.png.webp\" alt=\"lowendtutorial\" width=\"271\" height=\"70\"></a></p><p>If you want to learn how to build your own web crawler using a VPS, have you considered using Scrapy? In this installment of LowEndTutorials, we\u2019ll go over the basic functions of the Scrapy web crawling app.</p><p>Scrapy is an open source application that is used to extract data from websites. Its framework is developed in Python which enables your VPS to perform crawling tasks in a fast, simple and extensible way.</p><p><span id=\"more-9474\"></span></p><p><strong>How to Install Scrapy on Ubuntu 16.04 LTS</strong></p><p>As we previously mentioned, Scrapy is dependent on Python, development libraries and pip software.</p><p>Python\u2019s latest version should be pre-installed on your Ubuntu VPS. &nbsp;From there, we will only have to install pip and python developer libraries before installation of Scrapy.</p><p>Before continuing let\u2019s make sure that our system is up to date. Let\u2019s therefore log into our system and gain root privileges using the following command:</p><pre class=\"code\">&gt; sudo -i\n</pre><p>We can now make sure everything is up to date using the two following commands:</p><pre class=\"code\">&gt; apt-get update\n\n&gt; apt-get install python\n</pre><p>In the next step we are going to install Pip. Pip is the replacement for easy_install for python package indexer. It is used for installation and management of Python packages. We can perform that installation using the following command:</p><pre class=\"code\">&gt; apt-get install python-pip\n</pre><p>Once Pip is installed, we will have to install python development libraries by using following command.</p><pre class=\"code\">&gt; apt-get install python-dev\n</pre><p>If this package is missing, the installation of Scrapy will generate an error about the python.h header file. Make sure to check the output of the previous command before continuing with the next steps of the installation.</p><p>Scrapy framework can be installed from a deb package. Try running the following command:</p><pre class=\"code\">&gt; pip install scrapy\n</pre><p>The installation will take some time and should end with the following message:</p><pre class=\"code\"><em>\u201cSuccessfully installed scrapy queuelib service-identity parsel w3lib PyDispatcher cssselect Twisted pyasn1 pyasn1-modules attrs constantly incremental</em>\n\n<em>Cleaning up...\u201d</em>\n</pre><p>If you see that, you have successfully installed Scrapy and you are now ready to start crawling the web!</p><p>Before you start scraping, you will have to set up a new Scrapy project. Enter a directory where you\u2019d like to store your code and run:</p><pre class=\"code\">&gt; scrapy startproject myProject\n</pre><p>This will create a \u201cmyProject\u201d directory with the following content:</p><pre class=\"code\">- scrapy.cfg - the project configuration file - myProject/\n\n- you'll import your code from here\n\n- items.py - project items definition file\n\n- pipelines.py - project pipelines file\n\n- settings.py - project settings file\n\n- spiders/ - a directory where you'll later put your spiders\n</pre><p>We are now going to create our first spider and execute it to collect some information from the web.</p><p>Spiders are classes that you define. Scrapy uses spiders to scrape information from a website (or a group of websites). This is the code for our first Spider. Save it in a file named \u201cquotes_spider.py\u201d under the \u201cmyProject/spiders\u201d directory in your project:</p><pre class=\"code\"><strong>import</strong> scrapy\n\n<strong>class</strong> <strong>QuotesSpider</strong>(scrapy<strong>.</strong>Spider):\n\nname <strong>=</strong> \"quotes\"\n\n<strong>def</strong> <strong>start_requests</strong>(self):\n\nurls <strong>=</strong> [\n\n'http://quotes.toscrape.com/page/1/',\n\n'http://quotes.toscrape.com/page/2/',\n\n]\n\n<strong>for</strong> url <strong>in</strong> urls:\n\n<strong>yield</strong> scrapy<strong>.</strong>Request(url<strong>=</strong>url, callback<strong>=</strong>self<strong>.</strong>parse)\n\n<strong>def</strong> <strong>parse</strong>(self, response):\n\npage <strong>=</strong> response<strong>.</strong>url<strong>.</strong>split(\"/\")[<strong>-</strong>2]\n\nfilename <strong>=</strong> 'quotes-%s.html' <strong>%</strong> page\n\n<strong>with</strong> open(filename, 'wb') <strong>as</strong> f:\n\nf<strong>.</strong>write(response<strong>.</strong>body)\n\nself<strong>.</strong>log('Saved file %s' <strong>%</strong> filename)\n</pre><p>What this code will do is basically navigate the two following webpages that contain quotes from different authors and save them in html files named, quote-1.html and quote-2.html:</p><p><a href=\"http://quotes.toscrape.com/page/1/\">http://quotes.toscrape.com/page/1/</a></p><p><a href=\"http://quotes.toscrape.com/page/2/\">http://quotes.toscrape.com/page/2/</a></p><p>Once you have saved the file with the code you are ready to execute your first crawler using the two following commands:</p><pre class=\"code\">&gt; cd myProject\n\n&gt; scrapy crawl quotes\n</pre><p>The execution of the spider should end with the following line:</p><pre class=\"code\">&nbsp;\n\n<em>\u201c\u2026..[scrapy] INFO: Spider closed (finished)\u201d</em>\n</pre><p>If you list the files in your current directory you should see the new html files generated by the spider:</p><pre class=\"code\">&nbsp;\nquotes-1.html\n\nquotes-2.html\n</pre><p>In the following example we are going to extract the information of each author, following the links to their page and save the result in a JSON Lines format file. We will first need to create a new spider named author_spider.py with the following content:</p><pre class=\"code\">&nbsp;\n<strong>import</strong> scrapy\n\n<strong>class</strong> <strong>AuthorSpider</strong>(scrapy<strong>.</strong>Spider):\n\nname <strong>=</strong> 'author'\n\nstart_urls <strong>=</strong> ['http://quotes.toscrape.com/']\n\n<strong>def</strong> <strong>parse</strong>(self, response):\n\n\n<em># follow links to author pages</em>\n\n<strong>for</strong> href <strong>in</strong> response<strong>.</strong>css('.author+a::attr(href)')<strong>.</strong>extract():\n\n<strong>yield</strong> scrapy<strong>.</strong>Request(response<strong>.</strong>urljoin(href),\n\ncallback<strong>=</strong>self<strong>.</strong>parse_author)\n&nbsp;\n\n<em># follow pagination links</em>\n\nnext_page <strong>=</strong> response<strong>.</strong>css('li.next a::attr(href)')<strong>.</strong>extract_first()\n\n<strong>if</strong> next_page <strong>is</strong> <strong>not</strong> None:\n\nnext_page <strong>=</strong> response<strong>.</strong>urljoin(next_page)\n\n<strong>yield</strong> scrapy<strong>.</strong>Request(next_page, callback<strong>=</strong>self<strong>.</strong>parse)\n\n\n<strong>def</strong> <strong>parse_author</strong>(self, response):\n\n<strong>def</strong> <strong>extract_with_css</strong>(query):\n\n<strong>return</strong> response<strong>.</strong>css(query)<strong>.</strong>extract_first()<strong>.</strong>strip()\n\n\n<strong>yield</strong> {\n\n'name': extract_with_css('h3.author-title::text'),\n\n'birthdate': extract_with_css('.author-born-date::text'),\n\n'bio': extract_with_css('.author-description::text'),\n\n}\n\n</pre><p>We can now execute this new crawler with the following command:</p><pre class=\"code\">&gt; scrapy crawl author -o author.jl\n</pre><p>This will create a file named author.jl with the content of the extraction. The JSON Lines format is useful because it\u2019s stream-like, you can easily append new records to it.</p><p>This is just a brief overview of the Scrapy app. It looks like you could do perform some pretty sophisticated tasks using Scrapy on your Ubuntu VPS.</p><p>If you\u2019d like to learn more about Scrapy, the best thing to do is to take a deep dive into <a href=\"https://doc.scrapy.org/en/latest/index.html#section-basics\">Scrapy\u2019s documentation.</a></p><p class=\"dpsp-share-text\"></p><div id=\"dpsp-content-bottom\" class=\"dpsp-content-wrapper dpsp-shape-rounded dpsp-column-auto dpsp-has-spacing dpsp-no-labels dpsp-show-on-mobile dpsp-button-style-1 dpsp-has-icon-background dpsp-has-button-background dpsp-show-total-share-count dpsp-show-total-share-count-after\"><ul class=\"dpsp-networks-btns-wrapper dpsp-networks-btns-content dpsp-has-button-icon-animation\"><li><a rel=\"nofollow\" href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Flowendbox.com%2Fblog%2Fhow-to-build-your-own-web-crawler-using-an-ubuntu-vps%2F&amp;t=How%20to%20Build%20Your%20Own%20Web%20Crawler%20Using%20an%20Ubuntu%20VPS\" class=\"dpsp-network-btn dpsp-facebook dpsp-no-label dpsp-first\" title=\"Share on Facebook\"><span class=\"dpsp-network-icon\"></span><span class=\"dpsp-network-label-wrapper\"></span></a></li><li><a rel=\"nofollow\" href=\"https://twitter.com/intent/tweet?text=How%20to%20Build%20Your%20Own%20Web%20Crawler%20Using%20an%20Ubuntu%20VPS&amp;url=https%3A%2F%2Flowendbox.com%2Fblog%2Fhow-to-build-your-own-web-crawler-using-an-ubuntu-vps%2F&amp;via=LowEndNetwork\" class=\"dpsp-network-btn dpsp-twitter dpsp-no-label\" title=\"Share on Twitter\"><span class=\"dpsp-network-icon\"></span><span class=\"dpsp-network-label-wrapper\"></span></a></li><li><a rel=\"nofollow\" href=\"https://www.linkedin.com/shareArticle?url=https%3A%2F%2Flowendbox.com%2Fblog%2Fhow-to-build-your-own-web-crawler-using-an-ubuntu-vps%2F&amp;title=How%20to%20Build%20Your%20Own%20Web%20Crawler%20Using%20an%20Ubuntu%20VPS&amp;mini=true\" class=\"dpsp-network-btn dpsp-linkedin dpsp-no-label dpsp-last\" title=\"Share on LinkedIn\"><span class=\"dpsp-network-icon\"></span><span class=\"dpsp-network-label-wrapper\"></span></a></li></ul><div class=\"dpsp-total-share-wrapper\"><span class=\"dpsp-icon-total-share\"></span><span class=\"dpsp-total-share-count\">3</span><span>shares</span></div></div><div class=\"relpost-thumb-wrapper\"><div class=\"relpost-thumb-container\"><h3>Related posts:</h3><div style=\"clear: both\"></div><div style=\"clear: both\"></div><div class=\"relpost-block-container\"><a class=\"relpost-block-single\" href=\"https://lowendbox.com/blog/how-to-disable-ipv6-on-ubuntu/\"><div style=\"width: 150px; height: 215px;\"><div class=\"relpost-block-single-image\" alt=\"how to disable ipv6 on your ubuntu\" style=\"background: transparent url(https://lowendbox.com/wp-content/uploads/2020/05/how-to-disable-ipv6-ubuntu-150x150.jpg) no-repeat scroll 0% 0%; width: 150px; height: 150px;\"></div><div class=\"relpost-block-single-text\" style=\"font-family: Arial;  font-size: 12px;  color: #333333;\">How to Disable IPv6 on Ubuntu</div></div></a><a class=\"relpost-block-single\" href=\"https://lowendbox.com/blog/server-space-1gb-windows-vps-in-nj-or-amsterdam-for-6-mo-plus-and-exclusive-coupon/\"><div style=\"width: 150px; height: 215px;\"><div class=\"relpost-block-single-image\" alt=\"Server Space - 1GB Windows VPS in NJ or Amsterdam for $6/mo plus and Exclusive Coupon\" style=\"background: transparent url(https://lowendbox.com/wp-content/uploads/2020/04/serverspace2-150x150.jpg) no-repeat scroll 0% 0%; width: 150px; height: 150px;\"></div><div class=\"relpost-block-single-text\" style=\"font-family: Arial;  font-size: 12px;  color: #333333;\">Server Space - 1GB Windows VPS in NJ or Amsterdam for $6/mo plus and Exclusive Coupon</div></div></a><a class=\"relpost-block-single\" href=\"https://lowendbox.com/blog/hostigger-openstack-kvm-in-istanbul-plans-start-at-11-94-yr/\"><div style=\"width: 150px; height: 215px;\"><div class=\"relpost-block-single-image\" alt=\"Hostigger \u2013 Openstack/KVM in Istanbul! Plans start at $11.94/yr!\" style=\"background: transparent url(https://lowendbox.com/wp-content/uploads/2020/04/leb-centered-small.jpg) no-repeat scroll 0% 0%; width: 150px; height: 150px;\"></div><div class=\"relpost-block-single-text\" style=\"font-family: Arial;  font-size: 12px;  color: #333333;\">Hostigger \u2013 Openstack/KVM in Istanbul! Plans start at $11.94/yr!</div></div></a><a class=\"relpost-block-single\" href=\"https://lowendbox.com/blog/hostigger-goodbye2019-usa-nl-tr-location-3-core-10gb-ram-80gb-ssd-10tb-bw-250-mbps-network-speed-9-99-mo-or-75-yr/\"><div style=\"width: 150px; height: 215px;\"><div class=\"relpost-block-single-image\" alt=\"Hostigger - GoodBye2019 - USA/NL/TR Location | 3 Core, 10GB RAM, 80GB SSD, 10TB BW, 250 Mbps Network...\" style=\"background: transparent url(https://lowendbox.com/wp-content/uploads/2020/04/leb-centered-small.jpg) no-repeat scroll 0% 0%; width: 150px; height: 150px;\"></div><div class=\"relpost-block-single-text\" style=\"font-family: Arial;  font-size: 12px;  color: #333333;\">Hostigger - GoodBye2019 - USA/NL/TR Location | 3 Core, 10GB RAM, 80GB SSD, 10TB BW, 250 Mbps Network...</div></div></a></div><div style=\"clear: both\"></div></div></div><div id=\"author-bio-box\" style=\"background: #f8f8f8; border-top: 2px solid #cccccc; border-bottom: 2px solid #cccccc; color: #333333\"><h3><a style=\"color: #555555;\" href=\"https://lowendbox.com/blog/author/jbiloh/\" title=\"All posts by Jon Biloh\" rel=\"author\">Jon Biloh</a></h3><div class=\"bio-gravatar\"><img data-lazyloaded=\"1\" src=\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI3MCIgaGVpZ2h0PSI3MCIgdmlld0JveD0iMCAwIDcwIDcwIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjY2ZkNGRiIi8+PC9zdmc+\" alt=\"\" data-src=\"https://secure.gravatar.com/avatar/f766d6dc4b5f406b9f3cb0339d48de95?s=70&amp;d=retro&amp;r=pg\" data-srcset=\"https://secure.gravatar.com/avatar/f766d6dc4b5f406b9f3cb0339d48de95?s=140&amp;d=retro&amp;r=pg 2x\" class=\"avatar avatar-70 photo\" height=\"70\" width=\"70\" loading=\"lazy\"></div><a target=\"_blank\" rel=\"nofollow noopener noreferrer\" href=\"https://www.lowendtalk.com/profile/jbiloh\" class=\"bio-icon bio-icon-website\"></a><a target=\"_blank\" rel=\"nofollow noopener noreferrer\" href=\"http://Jbiloh\" class=\"bio-icon bio-icon-twitter\"></a><a target=\"_blank\" rel=\"nofollow noopener noreferrer\" href=\"https://www.linkedin.com/in/jon-biloh-4b32598/\" class=\"bio-icon bio-icon-linkedin\"></a><p class=\"bio-description\">I'm Jon Biloh and I own LowEndBox and LowEndTalk. I've spent my nearly 20 year career in IT building companies and now I'm excited to focus on building and enhancing the community at LowEndBox and LowEndTalk.</p></div></div><div class=\"feedback\"></div></div>"}